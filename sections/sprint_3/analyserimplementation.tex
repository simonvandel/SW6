\subsection{Implementation of the Analyser}\label{s3:analyser_implementation}

In order to fully understand the need for a good implementation of the analyser, we start with a quick run-down of the complexity of the unoptimised algorithm implemented directly as derived in \cref{subsec:estimatingCrowdFactors}. The pseudo code implementation can be seen in \cref{alg:unoptimised_algorithm}.
\jenote{Overvej om algoritmen skal ned i bilag (dvs at sektionen skal skrives lidt om)}
\begin{center}
\captionof{algorithm}{Unoptimised algorithm}\label{alg:unoptimised_algorithm}
\begin{algorithmic}[1]

\Function{Analyser}{List points, List people, Bandwidth h}
\State Densities $\gets$ empty List
\State Velocities $\gets$ empty List
\State Turbulence $\gets$ empty List
\State Pressure $\gets$ empty List
\ForAll{points, x}
    \State KernelSum $\gets 0$
    \ForAll{people, P}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{|x - P|}
    \EndFor
    \State \Call{Densities.Add}{KernelSum $\cdot\; 2 / (\pi \cdot h^2)$}
\EndFor

\ForAll{points, x}
    \State KernelSum $\gets 0$
    \State VelocitySum $\gets 0$
    \ForAll{people, P}
        \State VelocitySum $\gets$ VelocitySum $+$ \Call{VelocityOf}{P} $\cdot$ \Call{Kernel}{|x - P|}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{|x - P|}
    \EndFor
    \State \Call{Velocities.Add}{VelocitySum $/$ KernelSum}
\EndFor

\ForAll{points, x}
    \State KernelSum $\gets 0$
    \State TurbuSum $\gets 0$
    \ForAll{People, P}%
        \State TurbuSum $\gets$ TurbuSum $+$ \Call{DirectionOf}{P} $\cdot$ \Call{Kernel}{|x - P|}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{|x - P|}
    \EndFor
    \State \Call{Turbulence.Add}{TurbuSum $/$ KernelSum}
\EndFor

\ForAll{points, x}
    \State KernelSum $\gets 0$
    \State PressSum $\gets 0$
    \ForAll{People, P}
        \State VelocityDiff $\gets$ |\Call{VelocityOf}{P} $-$ x.LocalVelocity|
        \State PressSum $\gets$ PressSum $+$ (VelocityDiff)$^2$ $\cdot$ \Call{Kernel}{|x - P|}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{|x - P|}
    \EndFor
    \State \Call{Pressure.Add}{x.LocalDensity $\cdot$ (PressSum $/$ KernelSum)}
\EndFor
\State\textbf{Return} (points, Densities, Velocities, Turbulence, Pressure)
\EndFunction
\end{algorithmic}
\end{center}

This algorithm has an asymptotic time complexity of $$\mathcal{O}(\text{points} \cdot \text{people})$$ which at first might seem decent. However, recall that the amount of points is related to the resolution, meaning that a point for every square meter at SmukFest, will result in 230,000 points\cite{smukFacts}. Combining this with potentially 50,000 festival guests\cite{smukFacts}, we reach an amount of potential calculation, for each analysis, that a regular server will struggle to evaluate within the real-time requirement of \cref{sec:s3_requirements}. We therefore want to reduce the complexity as much as possible.


\subsubsection{Reducing Complexity}

The first thing we need to reduce is the complexity. This would mean reducing either of the terms of the complexity, namely points or people.

All points have to be considered since these are the points we want to find the local crowd factors for. We could reduce the amount of points we want to analyse on, but this would not reduce the complexity of the analyser itself. If we want to reduce the complexity we need to reduce the amount of people iterated through per point. Considering that the kernel function will return zero for any person further away from the point than the bandwidth, we can reduce the complexity given that a reasonable bandwidth is used. This also relies on the reasonable assumption that people take up space, and that therefore we can only have a certain amount people inside a given bandwidth.

Before we can utilise the drop off point of the kernel function, we need a way to check if a person is further away than the bandwidth, without having to iterate through all the people. This can be done using R-trees. An R-tree is a data structure that takes $\mathcal{O}(n)$ to build, and allows us to do range searches in $\mathcal{O}(log(n))$ \cite{rtree}. If we start the analysis with building an r-tree of the people, finding the people withing the bandwidth for each point would only take $\mathcal{O}(log(n))$ time to find, where n is the amount of people. The amount of people found using the range trees is here denoted with the function \emph{PeopleWithinBandwidth}. This would give us the complexity of $$\mathcal{O}(\text{people} + \text{points} \cdot log(\text{people}) * \text{PeopleWithinBandwidth(h)})$$

C

Considering that there is a physical limit as too how dense people can be packed, the function would become \emph{PeopleWithinBandwidth(h)} would return $h^2 \cdot \pi \cdot \text{maxDensity}$, which represents the maximal amount of people will have to considered for each point. Since we at runtime will have a constant bandwidth, $h$, and the max density is also constant, $PeopleWithinBandwidth(h)$ will have a constant worst case. The final complexity is therefore $$\mathcal{O}(\text{points} \cdot log(\text{people}))$$

\subsubsection{Parallel Computation}

Another way to decrease the time spent in on the analysis, is to take advantage of the embarrassingly parallel problem of calculating point. Since none of the calculations done for any individual point affect the the other points, we can calculate their values in parallel, meaning that we in principle can have an individual thread for each point.

\subsubsection{An Optimised Algorithm}
All these considerations leads us to the pseudo-code for the optimised analyser algorithm. The algorithm is split into two functions and can be seen in \cref{alg:revised_algorithm}. The first function on \cref{firstfunc} is the analyser function. This is basically a for-loop with two objectives: Find the relevant people for each point through an R-tree, and call the \emph{analysePoint} function. This for-loop represents the embarrassingly parallel part of the problem. It should be noted that while the same R-tree is used for multiple points, only read actions are done, and as such there are no race conditions.

The second function on \cref{secondfunc} of the algorithm, is the part where the local crowd factors are calculated. The primary change here, is the reduction of four for-loops, down to two. The primary reason for this reduction is the repeated calls to the \emph{Kernel} function, which calculates a given persons Kernel value. Since this is a rather costly calculation, we store the result of the calculation on \cref{tmpkernel}, and use this value for each factor. Note that on \cref{secondkernel}, another call is made to the \emph{Kernel} function. In the actual implementation this is avoided through a technical workaround in the first for-loop. However we have omitted that detail in order to keep the pseudo-code less complicated.

On line \cref{firstval} through \ref{lastval} we introduce summation variables, which will store the non-normalised values. Note that the value for velocity on \cref{veloval} is a vector. We then reach the first for-loop, which will iterate through every person in the reduced list, and add that person's crowd factors to the respective accumulator after it has been adjusted by the kernel value.

At \cref{validhead} we have an \emph{if} statement, which makes sure that the heading direction is not invalid, in accordance with the solution presented in \cref{subsub:turbu}, to the problem of still standing people. If the heading direction is valid, the accumulator is updated on \cref{adjustturbu} and \ref{turbusum}, along with the separate Kernel value summation on \cref{turbukernel}.

On \cref{nullcheck} we have an \emph{if} statement, which checks if the point has a Kernel value of $0$, which would cause a division error in \cref{secondres,lastpres}. The same principle goes for the \emph{if} on \cref{turbunullcheck} and the division on \cref{thirdres}.
Then at \cref{firstres} through \ref{thirdres} the first three factors are normalised, and on \cref{firstpres} through \ref{lastpres} we calculate the local crowd pressure. We finally return the result on \cref{return}.

This algorithm holds the desired complexity of $$\mathcal{O}(\text{points} \cdot log(\text{people}))$$

assuming the previously found constant maximum of people within an area.

\begin{center}
\captionof{algorithm}{Optimised algorithm using RTree range search}\label{alg:revised_algorithm}
\begin{algorithmic}[1]

\Function{Analyser}{List points, rTree t, Bandwidth h}\label{firstfunc}
\State Result $\gets$ Empty List 
\ForAll{points, p}
    \State CulledPeople $\gets$ \Call{t.inRange}{h,p} 
    \State Result $\gets$ Result + \Call{analysePoint}{p,CulledPeople}
\EndFor
\State \textbf{Return} Result

\EndFunction

\Function{analysePoint}{Point x, List people}\label{secondfunc}


\State KernelValSum $\gets 0$\label{firstval}
\State NonNormVeloSum $\gets vector(0.0,0.0)$\label{veloval}
\State NonNormTurbuSum $\gets 0$
\State TurbuKernelValSum $\gets 0$\label{lastval}


\ForAll{People, P}
    \State TmpKernelVal $\gets$ \Call{Kernel}{|x - P|}\label{tmpkernel}
    \State KernelValSum $\gets$ KernelValSum + TmpKernelVal
    \State NonNormVeloSum $\gets$ NonNormVeloSum + P.Velocity * TmpKernelVal
    \If{P.HeadingDirection is valid}\label{validhead}
        \State AdjustedTurbu $\gets$ P.HeadingDirection $\cdot$ TmpKernelVal\label{adjustturbu}
        \State NonNormTurbuSum $\gets$ NonNormTurbuSum $+$ Adjusted Turbu \label{turbusum}
        \State TurbuKernelValSum $\gets$ TurbuKernelValSum + TmpKernelVal\label{turbukernel}
    \EndIf
\EndFor

\If{KernelValSum is 0}\label{nullcheck}
    \State \textbf{Return} (0,(0,0),0,0)
\Else
    \State Density $\gets$ KernelValSum $\cdot \; 2 / (\pi \cdot h^2)$\label{firstres}
    \State Velocity $\gets$ NonNormVelocitySum / KernelValSum \label{secondres}
    \State Turbulence $\gets$ 0
    \If{TurbuKernelValSum is 0} \label{turbunullcheck}
        \State Turbulence $\gets$ 1 - ( NonNormTurbuSum / TurbuKernelValSum )\label{thirdres}
    \EndIf
    \State VelocityDiffSum $\gets$ 0 \label{firstpres}
    \ForAll{People, P}
        \State VelocityDiff $\gets$ |P.velocity - Velocity|$^2 \cdot$ \Call{Kernel}{|x - P|}\label{secondkernel}
        \State VelocityDiffSum $\gets$ VelocityDiffSum $+$ VelocityDiff
    \EndFor
    \State Pressure $\gets$ Density $\cdot$ ( VelocityDiffSum / KernelValSum )\label{lastpres}
\State \textbf{Return} (x, Density, Velocity, Turbulence, Pressure)\label{return}
\EndIf
\EndFunction
\end{algorithmic}
\end{center}






