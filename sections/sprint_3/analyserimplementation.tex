\subsection{Implementation of the Analyser}\label{s3:analyser_implementation}

In order to fully understand the need for a good implementation of the analyser, we start with a quick rundown of the complexity of the unoptimized direct analysis algorithm, which follows the mathematical formulas.

\begin{algorithmic}[1]

\Function{Analyser}{List of Points, List of People, Bandwidth}
\ForAll{points, x}
    \State KernelSum $\gets 0$
    \ForAll{People, P}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{P}
    \EndFor
    \State LocalDensity $\gets$ KernelSum $/$ (\Call{Count}{P} $\cdot$ Bandwidth)
\EndFor

\ForAll{points, x}
    \State KernelSum $\gets 0$
    \State VelocitySum $\gets 0$
    \ForAll{People, P}
        \State VelocitySum $\gets$ VelocitySum $+$ \Call{VelocityOf}{P} $\cdot$ \Call{Kernel}{P}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{P}
    \EndFor
    \State x.LocalVelocity $\gets$ VelocitySum $/$ KernelSum
\EndFor

\ForAll{points, x}
    \State KernelSum $\gets 0$
    \State TurbuSum $\gets 0$
    \ForAll{People, P}%
        \State TurbuSum $\gets$ TurbuSum $+$ \Call{DirectionOf}{P} $\cdot$ \Call{Kernel}{P}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{P}
    \EndFor
    \State LocalTurbulence $\gets$ TurbuSum $/$ KernelSum
\EndFor

\ForAll{points, x}
    \State KernelSum $\gets 0$
    \State PressSum $\gets 0$
    \ForAll{People, P}
        \State VelocityDif $\gets$ (\Call{VelocityOf}{P} $-$ x.LocalVelocity)
        \State PressSum $\gets$ PressSum $+$ \Call{Abs}{VelocityDif}$^2$ $\cdot$ \Call{Kernel}{P}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{P}
    \EndFor
    \State LocalPressure $\gets$ x.LocalDensity $\cdot$ (PressSum $/$ KernelSum)
\EndFor
\EndFunction
\end{algorithmic}

%Create points to cover the area.
%
%For each point x:
%    KernelSum <- 0
%    For each person p:
%        KernelSum += KernelFunction(p)
%    x.LocalDensity <- KernelSum / (people * bandwidth)
%    
%For each point x:
%    VelocitySum <- 0
%    KernelSum <- 0
%    For each person p:
%        VelocitySum += VelocityOf(p) * KernelFunction(p)
%        KernelSum += KernelFunction(p)
%    x.LocalVelocity <- VelocitySum / KernelSum
%   
%For each point x:
%    DirectionSum <- 0
%    KernelSum <- 0
%   For each person p:
%       DirectionSum += HeadingDirectionOf(p) * KernelFunction(p)
%        KernelSum += KernelFunction(p)
%    x.LocalTurbulence <- absoluteOf( DirectionSum / KernelSum )
%
%For each point x:
%    pressureSum <- 0
%    KernelSum <- 0
%    For each person p:
%        pressureSum += 
%        kernelSum += kernelFunction(p)
%    x.LocalPressure <- x.LocalDensity * (pressureSum / KernelSum)
%    
%Return Points

This algorithm has a running time of $$O(4\cdot((\text{points} \cdot \text{people}) \cdot 2 \cdot O(KernelFunction)))$$ which might seem as a decent complexity, but considering the constants and the amount of points and people, we need to reduce this.


\subsubsection{Reducing Complexity}

The first thing we need to reduce is the complexity. Removing constant from the notation the complexity itself comes out to $$O(\text{points} \cdot \text{people})$$ which prompts us to reduce either the amount of points or the amount of people.

points can obviously be reduced, but not without also reducing the granularity of the result. But even if we reduce the number of points, we are not able to reduce the complexity. So if we want to reduce the complexity we need to reduce the amount of people iterated through. Considering that the kernel function will return zero for any person further away from the point than the bandwidth, we can reduce the complexity given that a reasonable bandwidth is used. This also relies on the reasonable assumption that people take up space, and that therefore we can only have a certain amount people inside a given bandwidth.
\kanote{færdiggør analysen når vi har en bandwidth}

But before we can utilize this, we need a way to check if a person is further away than the bandwidth, without having to iterate through all the people. Luckily such a way exists in the form of R-trees. An R-tree is a data structure that allows us to do range searches in $O(log_m(n))$ \cite{} \kanote{R-tree kilde + overvej at uddybe lidt}.


\subsubsection{Reducing Constants}

Having reduced the complexity, we still need to reduce the amount of constants in our algorithms run time. We wish to reduce the amount of iterations on the points, as well repeated calculations such as the Kernel values.

%Complexitet
%%

%Minimering af konstanter
%% R-trees
%% Hvorfor det er nice

% Hvorfor regner vi ikke tingene ud hver for sig?