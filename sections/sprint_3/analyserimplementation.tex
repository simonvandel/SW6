\subsection{Implementation of the Analyser}\label{sec:kernelDensityEstimation}

In order to fully understand the need for a good implementation of the analyser, we start with a quick rundown of the complexity of the unoptimized direct analysis algorithm seen in \cref{alg:unoptimised_algorithm}, which follows the mathematical formulas in \cref{sub:kernelDensityEstimation}.

\begin{center}
\captionof{algorithm}{Unoptimised algorithm}\label{alg:unoptimised_algorithm}
\begin{algorithmic}[1]

\Function{Analyser}{List points, List people, Bandwidth h}
\State Densities $\gets$ empty List
\State Velocities $\gets$ empty List
\State Turbulence $\gets$ empty List
\State Pressure $\gets$ empty List
\ForAll{points, x}
    \State KernelSum $\gets 0$
    \ForAll{people, P}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{|x - P|}
    \EndFor
    \State \Call{Densities.Add}{KernelSum $\cdot\; 2 / (\pi \cdot h^2)$}
\EndFor

\ForAll{points, x}
    \State KernelSum $\gets 0$
    \State VelocitySum $\gets 0$
    \ForAll{people, P}
        \State VelocitySum $\gets$ VelocitySum $+$ \Call{VelocityOf}{P} $\cdot$ \Call{Kernel}{|x - P|}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{|x - P|}
    \EndFor
    \State \Call{Velocities.Add}{VelocitySum $/$ KernelSum}
\EndFor

\ForAll{points, x}
    \State KernelSum $\gets 0$
    \State TurbuSum $\gets 0$
    \ForAll{People, P}%
        \State TurbuSum $\gets$ TurbuSum $+$ \Call{DirectionOf}{P} $\cdot$ \Call{Kernel}{|x - P|}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{|x - P|}
    \EndFor
    \State \Call{Turbulence.Add}{TurbuSum $/$ KernelSum}
\EndFor

\ForAll{points, x}
    \State KernelSum $\gets 0$
    \State PressSum $\gets 0$
    \ForAll{People, P}
        \State VelocityDiff $\gets$ |\Call{VelocityOf}{P} $-$ x.LocalVelocity|
        \State PressSum $\gets$ PressSum $+$ (VelocityDiff)$^2$ $\cdot$ \Call{Kernel}{|x - P|}
        \State KernelSum $\gets$ KernelSum $+$ \Call{Kernel}{|x - P|}
    \EndFor
    \State \Call{Pressure.Add}{x.LocalDensity $\cdot$ (PressSum $/$ KernelSum)}
\EndFor
\State\textbf{Return} (points, Densities, Velocities, Turbulence, Pressure)
\EndFunction
\end{algorithmic}
\end{center}

This algorithm has an asymptotic time complexity of $$\mathcal{O}(\text{points} \cdot \text{people})$$ which at first might seem decent. However, recall that the amount of points is related to the resolution, meaning that a point for every square meter at SmukFest, will result in 230.000 points\cite{smukFacts}. Combining this with potentially 50,000 festival guests\cite{smukFacts}, we reach an amount of potential calculation, for each analysis, that a regular server will struggle to evaluate within the real-time requirement of \cref{sec:s3_reqs}. We therefore want to reduce the complexity as much as possible.


\subsubsection{Reducing Complexity}

The first thing we need to reduce is the complexity. This would mean either the reduction of points or people.

Points can obviously be reduced, but not without also reducing the granularity of the result. But even if we reduce the number of points, we are not able to reduce the complexity. So if we want to reduce the complexity we need to reduce the amount of people iterated through. Considering that the kernel function will return zero for any person further away from the point than the bandwidth, we can reduce the complexity given that a reasonable bandwidth is used. This also relies on the reasonable assumption that people take up space, and that therefore we can only have a certain amount people inside a given bandwidth.

But before we can utilise this, we need a way to check if a person is further away than the bandwidth, without having to iterate through all the people. Luckily such a way exists in the form of R-trees. An R-tree is a data structure that takes $\mathcal{O}(n)$ to build, and allows us to do range searches in $\mathcal{O}(log(n))$ \cite{rtree}. If we start the analysis with building an r-tree of the people, each point would only have to query $\mathcal{O}(log(n)$ people, giving us the complexity of $$\mathcal{O}(\text{people} + \text{points} \cdot log(\text{people}) * PeopleWithinBandwidth(h))$$

Considering that there is a physical limit as too how dense people can be packed, the function $PeopleWithinBandwidth(h) = h^2 * \pi * maxDensity$. Since we at runtime will have a constant bandwidth, $h$, the final complexity will be $$\mathcal{O}(\text{points} \cdot log(\text{people}))$$.

\subsubsection{Parallel Computation}

Another way to increase the calculation of the analysis, is to take advantage of the embarrassingly parallel problem of calculating point. Since none of the calculations done for any individual point affect the the other points, we can calculate their values in parallel.

\subsubsection{An Optimised Algorithm}

All these considerations leads us to the pseudo-code for the optimised analyser algorithm. The algorithm is split into two functions and can be seen in \cref{alg:revised_algorithm}. The first function on line 1 is the analyser function. This is basically a for-loop with to objectives: Find the relevant people through an R-tree, and call the \emph{analysePoint} function. This for-loop represents the embarrassingly parallel part of the problem. It should be noted that while the same R-tree is used for multiple points, only read actions are done, and as such there are no race conditions.

The second function on line 9 of the algorithm, is the part where the local crowd densities are calculated. The primary change here, is the reduction of four for-loops, down to two. The primary reason for this reduction is the repeated calls to the \emph{Kernel} function, which calculates a given persons Kernel value. Since this is a rather costly calculation, we store the result of the calculation on line 15, and use this value for each factor. Note that on line 31, another call is made to the \emph{Kernel} function. In the actual implementation this is avoided through a technical workaround in the first for-loop. However we have omitted that detail in order to keep the pseudo-code less complicated.

On line 10 to 13 we introduce summation variables, which will store the non-normalised values. Note that the value for velocity on line 11 is a vector. We then reach the first for-loop, which will iterate through every person in the reduced list, and add that persons value to the accumulators after it has been adjusted by the kernel value.

At line 18 we have an \emph{if} statement, which makes sure that the heading direction is not invalid, in accordance with the solution presented in \cref{subsub:turbu}, to the problem of stilling standing people. If the heading direction is valid, the accumulator is updated along with the separate Kernel value summation.



\kanote{komment√©r koden}

%\LineComment{We reduce the amount of people using a range search}
%\LineComment{List of tuples containing individual Kernel values and velocity}
%\LineComment{Summarised Kernel variable adjusted for turbulence}

\begin{center}
\captionof{algorithm}{Optimised algorithm using RTree range search}\label{alg:revised_algorithm}
\begin{algorithmic}[1]

\Function{Analyser}{List points, rTree t, Bandwidth h}
\State Result $\gets$ Empty List 
\ForAll{points, p}
    \State CulledPeople $\gets$ \Call{t.inRange}{h,p} 
    \State Result $\gets$ Result + \Call{analysePoint}{p,CulledPeople}
\EndFor
\State \textbf{Return} Result

\EndFunction

\Function{analysePoint}{Point x, List people}


\State KernelValSum $\gets 0$
\State NonNormVeloSum $\gets vector(0.0,0.0)$
\State NonNormTurbuSum $\gets 0$
\State TurbuKernelValSum $\gets 0$ 


\ForAll{People, P}
    \State TmpKernelVal $\gets$ \Call{Kernel}{|x - P|}
    \State KernelValSum $\gets$ KernelValSum + TmpKernelVal
    \State NonNormVeloSum $\gets$ NonNormVeloSum + Velocity * TmpKernelVal
    \If{P.H-Direction is valid}
        \State AdjustedTurbu $\gets$ P.H-Direction $\cdot$ TmpKernelVal
        \State NonNormTurbuSum $\gets$ NonNormTurbuSum $+$ Adjusted Turbu 
        \State TurbuKernelValSum $\gets$ TurbuKernelValSum + TmpKernelVal
    \EndIf
\EndFor

\If{KernelValSum is 0}
    \State \textbf{Return} (0,(0,0),0,0)
\Else
    \State Density $\gets$ KernelValSum $\cdot \; 2 / (\pi \cdot h^2)$
    \State Velocity $\gets$ NonNormVelocitySum / KernelValSum
    \State Turbulence $\gets$ 1 - ( NonNormTurbuSum / TurbuKernelValSum )
    \State VelocityDiffSum $\gets$ 0
    \ForAll{People, P}
        \State VelocityDiff $\gets$ |P.velocity - Velocity|$^2 \cdot$ \Call{Kernel}{|x - P|}
        \State VelocityDiffSum $\gets$ VelocityDiffSum $+$ VelocityDiff
    \EndFor
    \State Pressure $\gets$ Density $\cdot$ ( VelocityDiffSum / KernelValSum )



\State \textbf{Return} (x, Density, Velocity, Turbulence, Pressure)
\EndIf
\EndFunction
\end{algorithmic}
\end{center}






